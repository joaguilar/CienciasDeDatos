---
title: "Tarea 3"
author: "Jose Aguilar, Eilyn Salazer, Crisia Piedra"
date: "May 6, 2020"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(arules)) install.packages("arules",dependencies = TRUE,repos=c(CRAN="https://cran.cnr.berkeley.edu/"))
library(arules)
if(!require(readxl)) install.packages("readxl",dependencies = TRUE,repos=c(CRAN="https://cran.cnr.berkeley.edu/"))
library("readxl")
library(ggplot2)
library(knitr)
if(!require(kableExtra)) install.packages("kableExtra",dependencies = TRUE,repos=c(CRAN="https://cran.cnr.berkeley.edu/"))
library(kableExtra)
library(arules)
library(arulesViz)

library(tidyverse)
if(!require(plyr)) install.packages("plyr",dependencies = TRUE)
library(plyr); library(dplyr)
library(ggplot2)
library(knitr)
library(lubridate)
if(!require(assertr)) install.packages("assertr",dependencies = TRUE)
library(assertr)
if(!require(reshape2)) install.packages("reshape2",dependencies = TRUE)
library(reshape2)

```

# Entendimiento del negocio

El problema a evaluar consiste en identificar patrones o caracteristicas en común de los miembros del cantón #21 de San José y basado en estás caracteristicas crear grupos de trabajo para sacar adelante los proyectos de interés del cantón.

## Objetivos de minería de datos 

*	Identificar las 3 mejores reglas que describen que atributos suelen reunir determinados miembros de la comunidad.

## Criterios de éxito desde la perspectiva de minería de datos

* Evaluar la confianza del modelo a priori con el fin de validar que la confianza de las reglas seleccionadas sea mayor al 90%.

#	Entendimiento de los datos

El conjunto de datos seleccionado cuenta con 3484 observaciones capturadas a través de la entrevista realizada por el alcalde el cual evalua 12 atributos o variables, las caracteristicas de estos datos recopilados se describen a continuación.

## Exploración de los datos 

El conjunto de datos encuesta.csv contiene los siguientes campos:

```{r warning=FALSE}
setwd('C:/Curso TEC/Tarea3/CienciasDeDatos/05_Mineria_De_Datos/Tarea3')
atributos<-read.csv('descripcion_atributos.csv', sep = ',',dec = '.')
kable(atributos[,0:3])

```

## Verificación de la calidad de datos

Para la evaluación de la calidad de los datos, se emplea la función summary para identificar la presencia de valores nulos, así como valores minimos y máximos de las variables númericas a fin de validar,
si a simple vista es posible encontrar outliers o valores fuera de rango, como valores negativos.

```{r warning=FALSE}
dataset<-read.csv('Encuesta.csv', sep = ',',dec = '.')
str(dataset)
summary(dataset)
```
 Como se puede observar en la tabla anterior, no se encontraron valores nulos ni negativos.
 
#	Preparación de los datos 

La primera tranformación de los datos consiste en convertir las columnas Trabaja y Familia a un valor numérico para que sea consistente con las demás variables:

```{r warning=FALSE}
# Lo convertimos usando as.integer(), pero debemos restarle 1 para que quede con dos valores 0 y 1, en vez de 1 y 2 como funciona al convertir de factor a entero
dataset$Trabaja1 <- as.integer(dataset$Trabaja)-1
dataset$Familia <- as.integer(dataset$Familia)-1
dataset$Familia<- factor(dataset$Familia,labels=c("No","Si"))
dataset$Trabaja<- factor(dataset$Trabaja,labels=c("No","Si"))
dataset$Hobbies<- factor(dataset$Hobbies,labels=c("No","Si"))
dataset$Club_Social<- factor(dataset$Club_Social,labels=c("No","Si"))
dataset$Politica<- factor(dataset$Politica,labels=c("No","Si"))
dataset$Profesional<- factor(dataset$Profesional,labels=c("No","Si"))
dataset$Medioambiente<- factor(dataset$Medioambiente,labels=c("No","Si"))
dataset$Grupo_Apoyo<- factor(dataset$Grupo_Apoyo,labels=c("No","Si"))
str(dataset)
```


## Graficos de distribución de los valores de las variables que son factores:

A fin de explorar la distribución de las variables se crean los siguientes gráficos. 

```{r warning=FALSE}
dataset_factors =dataset[,sapply(dataset, is.factor)]

for (f_name in colnames(dataset[,sapply(dataset, is.factor)])){
  print(f_name)
  barplot(prop.table(table(dataset[,f_name])),main = paste("Distribución de ",f_name),
          space = 1.2, axisnames = TRUE, col = rainbow(3))
}


```

## Selección de los datos 

Para seleccionar los datos a utilizar, vamos a revisar la distribución de los valores en las diferentes columnas utilizando el comando summary:

```{r warning=FALSE}
summary(dataset)
```

Revisando los datos, podemos observar que en dos variables _Club_Social_ y _Politica_ existe un desbalance importante en la distribución de los valores, con mas del 80% de las observaciones con un solo valor:

| Variable | Si (%) | No (%) |
|---|---|---|
| Club_Social | 10.36 | 89.64 |
| Politica | 18.87 | 81.13 |

Debido a esto, no seran consideramos para el análisis.

```{r warning=FALSE}
dataset$Club_Social <- NULL
dataset$Politica <- NULL
dataset$Trabaja1 <- NULL
```


## Limpieza de los datos 

Para asegurar el correcto funcionamiento del algoritmo, vamos a aplicar la función na.omit a fin de remover cualquier valor nulo, existe.

```{r warning=FALSE}
dataset<- na.omit(dataset)
```

## Construcción de nuevos datos (atributos) 

Se crearon dos nuevas variables como el rango de edad y el tiempo de respuesta para agrupar los datos. Para ambas variables se crearon 3 subgrupos. Una vez creados estas nuevas variables se procede a eliminar las variables originales.

```{r warning=FALSE}
dataset$Grupo_Edad<-discretize(dataset$Edad,method="frequency",breaks = 3, labels = c("Edad_Joven","Edad_Media","Edad_Mayor"))
levels(dataset$Grupo_Edad)

dataset$Grupo_Tiempo_Respuesta<-discretize(dataset$Tiempo_Respuesta,method="frequency",breaks = 3, 
                                           labels = c("Tiempo_Respuesta_Rapida","Tiempo_Respuesta_Media","Tiempo_Respuesta_Lenta"))
levels(dataset$Grupo_Tiempo_Respuesta)

dataset$Edad <- NULL
dataset$Tiempo_Respuesta <- NULL

str(dataset)

colnames(dataset)

```
En el caso de edad estos grupos son: Edad_Joven (De 17 a 30 años), Edad_Media (30 a 43 años) y Edad_Mayor (43 a 57 años)

En el caso de Tiempo_Respuesta los 3 subcojuntos son: Tiempo_Respuesta_Rapida de 2.01 a 4.56 minutos,Tiempo_Respuesta_Media de 4.56 a 7.24 minutos y Tiempo_Respuesta_Lenta de 7.24 a 10.15 minutos.

## Transformaciones aplicadas a los datos 
 
En este caso todo el desarrollo de la tarea se realiza a través de R. El detalle de las modificaciones se encuentra en los apartados Preparación de los datos: Selección de los datos, Limpieza de los datos
y Construcción de nuevos datos.
 
#	Fase de modelado  

## Selección de técnicas (en este caso A priori)

Para esta tarea se hace uso de la técnica A priori, tomando en consideración el conocimiento aprendido en clase y las instrucciones brindadas por la profesora para la creación de esta tarea.

## Construcción del modelo (en este caso serán conjuntos de reglas) 

### Modelo 1
####	Selección de los parámetros 

Para la creación del primer conjunto de reglas se hace uso de todas originales (excepto Politica y  Club Social) a fin de explorar los resultados de las reglas generadas.

```{r warning=FALSE}
#transacciones <- col_concat(dataset,sep=",")
transacciones<-as(dataset,Class = "transactions")
head(transacciones)
 
 # se guardan los datos como archivo csv en formato transaccion
write(transacciones,'transaccionesv1.csv',sep=',')
#write.csv(transacciones,'transaccionesv1.csv',quote = FALSE)

```


##### Exploración de las transacciones

Para explorar las transacciones creadas, se utiliza el comando summary, en el mismo es posible ver que los items más frecuentes son: Grupo_Apoyo=No, Profesional=No, Medioambiente=No, Hobbies=No y Familia=No

```{r}
summary(transacciones)
```

####	Ejecución (generación de reglas, eliminación de subconjuntos,etc) 

##### Generación de las reglas usando A priori

Vamos a utilizar un soporte del 0.1 (10%) para filtrar los items que tienen una baja frecuencia, es decir que tienen una baja frecuencia de aparición en la misma transacción. Igualmente utilizamos una confianza de 0.9 (90%) para incluir items con una alta probabilidad de ocurrencia.
```{r}
 # se generar las reglas usando el algoritmo a priori, con un soporte del 1% y una confianza de 0.9 o 90%
  reglas<- apriori(transacciones,parameter = list(supp=0.1,conf=0.9,maxlen=10))
  
  inspect(reglas[1:10])

  # eliminar reglas que son subconjuntos de otras 
  subconjuntos<- which(colSums(is.subset(reglas,reglas))>1)
  
  reglasFinal<- reglas[-subconjuntos]
  #inspect(reglasFinal[1:10])
  
  # se ordena por soporte y se observan las primeras 10 reglas
  inspect(sort(reglasFinal,by='lift',decreasing = TRUE)[1:10])
  
  # se filtran las reglas con confianza superior a 0.25
  mejoresReglas<- reglasFinal[quality(reglasFinal)$confidence>0.90]
  
```

```{r warning=FALSE}
# se leerá el archivo como transacciones formato basket
 transacciones<- read.transactions('transaccionesv1.csv',sep=',',format = 'basket')
 
```
####	Descripción general de las reglas obtenidas (incluya al menos un gráfico) 

##### Visualización de las 3 mejores reglas (según confianza) - grafo
```{r fig.align='center'}
   tresMejoresReglas<- head(mejoresReglas,n=3,by='lift')
   plot(tresMejoresReglas,method = 'graph',engine = 'htmlwidget')
```
##### Visualización de  3 reglas - gráfico paracoord
```{r}
# graficar 10 reglas individuales para observar lhs y rhs
  Top3<-head(mejoresReglas,n=3,by='confidence')
  plot(Top3,method ='paracoord')
```


#### Evaluación de los modelos 

#####	Muestre e interprete las mejores 3 reglas de cada conjunto generado 

Las mejores reglas al utilizar todas las siguientes:

```{r}
inspect(Top3)
```


### Modelo 2

##### Generación de las reglas usando A priori

Para el segundo modelo se utiliza un soporte más bajo de 0.02(2%), la confianza en 0.7 (70%) y el maxlen en 5, esto con el propósito de flexibilizar las restricciones del modelo, pero al mismo tiempo reducir el número de items empleadas por el algoritmo al generar las reglas, y asi comparar los resultados de ambos modelos

```{r}
 # se generarn las reglas usando el algoritmo a priori, con un soporte del 2% y una confianza de 0.7%

  reglasv2<- apriori(transacciones,parameter = list(supp=0.02,conf=0.7,maxlen=5))
  
  inspect(reglasv2[1:10])

  # eliminar reglas que son subconjuntos de otras 
  subconjuntosv2<- which(colSums(is.subset(reglasv2,reglasv2))>1)
  
  reglasFinalv2 <- reglasv2[-subconjuntosv2]
  #inspect(reglasFinal[1:10])
  
  # se ordena por soporte y se observan las primeras 3 reglas
  inspect(sort(reglasFinalv2,by='lift',decreasing = TRUE)[1:3])
  
  # se filtran las reglas con confianza superior a 0.25
  mejoresReglasv2<- reglasFinalv2[quality(reglasFinalv2)$confidence>0.70]
  
```

```{r warning=FALSE}
# se leerá el archivo como transacciones formato basket
 transaccionesv2 <- read.transactions('transaccionesv1.csv',sep=',',format = 'basket')
 
```
####	Descripción general de las reglas obtenidas (incluya al menos un gráfico) 

##### Visualización de las 3 mejores reglas (según confianza) - grafo
```{r fig.align='center'}
   tresMejoresReglasv2 <- head(mejoresReglasv2,n=3,by='lift')
   plot(tresMejoresReglasv2,method = 'graph',engine = 'htmlwidget')
```
##### Visualización de  3 reglas - gráfico paracoord
```{r}
# graficar 10 reglas individuales para observar lhs y rhs
  Top3v2<-head(mejoresReglasv2,n=3,by='confidence')
  plot(Top3v2,method ='paracoord')
```


#### Evaluación de los modelos 

#####	Muestre e interprete las mejores 3 reglas de cada conjunto generado 

Las mejores reglas al utilizar todas las siguientes:

```{r}
inspect(Top3v2)
```
